---
title: "Homework 6"
author: "Nathaniel Haulk"
date: "11/8/2021"
output: pdf_document
---
# Question 1

## Part A
```{r 1A}
## Data from the website
x = c(110.5, 105.4, 118.1, 104.5, 93.6, 84.1, 77.8, 75.6)
y = c(5.755, 5.939, 6.010, 6.545, 6.730, 6.750, 6.899, 7.862)

## Binds x and y into a dataframe
d = data.frame(c(x,y))

## Creates variable fit_d that shows the slope and the y-intercept
fit_d = lm(y~x, data = d)

print(fit_d)
```
The least squares estimate is of $\beta_1$ is -.037. This value represent the best fit of the trend of the data that reduces the distance from all of the points to the line itself

## Part B
```{r 1B}
## Runs an ANOVA test
anovad = anova(fit_d)

print(anovad)

## Used to get the t-test p-value
summaryd = summary(fit_d)

print(summaryd)
```

Both the F-test and the T-test show a P-value <. 01 confirming the alternative hypothesis. This means that $H_a\neq0$ is true. 

## Part C

```{r Q1C}
## Calculates the t_(n-2,a/2) value
print(qt(.05/2, 8-2))

## Calculate the confidence intervals
conf_d = confint(fit_d,)

```
The 95% confidence interval states that To calculate the 95% confidence interval for $\beta_0$, we use the equation:

$$\beta_0 \pm t_{n-2,a/2} * s.e.(\beta_0)$$

We know that $\beta_0 = 10.1375$, $s.e.(\beta_0) = .8423$ and that $t_{n-2,a/2} = -2.4469$ based on the R code above.

Plugging in these values gives us:

$$10.1375 \pm -2.4469 * .8423$$
which equals both 8.077 and 12.199.

## Part D

```{r Q1D}
## Calculates raw residuals
resid_d= resid(fit_d)

print(resid_d)

```


Based off the values from part a, we know the y-intercept and the slope of the regression line. This give us the equation:

$$ \hat{y} = 10.137 - 0.0372x$$

## Part E

```{r Q1E}

##Print the error values
print(summaryd[6])

```

Based off the summary statistics generated in part b, we know that $\hat{\sigma}^2 = .3624$

## Part F

```{r Q1F}
## Predicts the 95% confidence interval based on the new rice of x = 100
mu0conf = predict(fit_d, newdata = data.frame(x = 100), interval = "confidence")

print(mu0conf)
```

## Part G
```{r Q1G}
## Predicts the 95% prediction interval based on the new rice of x = 100
mu0pred = predict(fit_d, newdata = data.frame(x = 100), interval = "prediction")

print(mu0pred)
```

Compared to the confidence interval (range = 0.647), the prediction interval (range = 1.8879) is much wider

## Part H
```{r}
## Pulls the r^2 value from summaryd
d_r_squared = summaryd['r.squared']
```

The R-squared value represents how much variance that can be explained by the independent variable, in this case plant height. 

# Question 2

## Part A
```{r}
## Data for question 2
x2 = c(1, 2, 3, 4, 5, 6, 7, 8, 9)
y2 = c(-2.08, -0.72, 0.28, 0.92, 1.20, 1.12, 0.68, -0.12, -1.28)

# Combines data into dataframe
d2 = data.frame(x2,y2)

## Plots data
plot(x2,y2)
```

## Part B
```{r}
## Creates variable fit_d2 that shows the stats data from x2 and y2
fit_d2 = lm(y2~x2, data = d2)

summary_d2 = summary(fit_d2)

## Adds a residual column to d2
d2 = cbind(d2,summary_d2['residuals'])

plot(y2, d2[,3], ylab = 'residuals')
```

## Part C
```{r}
plot(x2, d2[,3], ylab = 'Residuals')
```


```{r}

yhat = predict(fit_d2, newdata = data.frame(x = x2))

plot(yhat, d2[,3], ylab = 'Residuals')
```

C and D show very similar graphs. This is because the patter will stay the same regardless if the residuals are being compared to the x-values or to the fitted values.

When comparing B vs D, d gives a better indication of the fact that there is no fit. B makes it seem as if there is a linear relation between the points, when there is obviously not. D shows that there is no obvious linear relationship, mocking a similar pattern to when the x-values are plotted against the y-values